"""
æ‰‹æ©Ÿè©•è«–æƒ…æ„Ÿåˆ†æ - æ–‡æœ¬é è™•ç†
Text Preprocessing and Feature Engineering
"""

# %% å°å…¥å¥—ä»¶
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from textblob import TextBlob
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import Counter
from wordcloud import WordCloud
import warnings
warnings.filterwarnings('ignore')

# ä¸‹è¼‰å¿…è¦çš„ NLTK è³‡æº
print("ğŸ“¥ ä¸‹è¼‰ NLTK è³‡æº...")
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('omw-1.4', quiet=True)
print("âœ… NLTK è³‡æºä¸‹è¼‰å®Œæˆ")

# è¨­å®š
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 8)
Path('visualizations').mkdir(exist_ok=True)
Path('data/processed').mkdir(parents=True, exist_ok=True)

print("\n" + "=" * 80)
print("æ–‡æœ¬é è™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹")
print("=" * 80)

# %% è¼‰å…¥æ•¸æ“š
print("\nğŸ“‚ è¼‰å…¥æ•¸æ“š...")
df = pd.read_csv('data/processed/enhanced_data.csv')
print(f"âœ… è¼‰å…¥ {len(df):,} æ¢è¨˜éŒ„")

# %% è­˜åˆ¥æ–‡æœ¬æ¬„ä½
print("\nğŸ” è­˜åˆ¥æ–‡æœ¬æ¬„ä½...")
text_cols = []
for col in df.columns:
    if df[col].dtype == 'object':
        avg_length = df[col].astype(str).str.len().mean()
        if avg_length > 50:
            text_cols.append(col)
            print(f"  - {col} (å¹³å‡é•·åº¦: {avg_length:.0f})")

if not text_cols:
    print("âŒ æœªæ‰¾åˆ°æ–‡æœ¬æ¬„ä½")
    exit()

# é¸æ“‡ä¸»è¦æ–‡æœ¬æ¬„ä½
main_text_col = text_cols[0]
print(f"\nâœ… ä¸»è¦æ–‡æœ¬æ¬„ä½: {main_text_col}")

# %% æ–‡æœ¬æ¸…æ´—å‡½æ•¸
def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    
    # è½‰å°å¯«
    text = str(text).lower()
    
    # ç§»é™¤ URL
    text = re.sub(r'http\S+|www.\S+', '', text)
    
    # ç§»é™¤ email
    text = re.sub(r'\S+@\S+', '', text)
    
    # ç§»é™¤ HTML æ¨™ç±¤
    text = re.sub(r'<.*?>', '', text)
    
    # ä¿ç•™å­—æ¯ã€æ•¸å­—å’Œç©ºæ ¼
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
    
    # ç§»é™¤å¤šé¤˜ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# %% é€²éšæ–‡æœ¬è™•ç†å‡½æ•¸
def preprocess_text(text, remove_stopwords=True, lemmatize=True):
    """é€²éšæ–‡æœ¬é è™•ç†"""
    # æ¸…æ´—
    text = clean_text(text)
    
    # åˆ†è©
    tokens = word_tokenize(text)
    
    # ç§»é™¤åœç”¨è©
    if remove_stopwords:
        stop_words = set(stopwords.words('english'))
        tokens = [w for w in tokens if w not in stop_words and len(w) > 2]
    
    # è©å½¢é‚„åŸ
    if lemmatize:
        lemmatizer = WordNetLemmatizer()
        tokens = [lemmatizer.lemmatize(w) for w in tokens]
    
    return ' '.join(tokens)

# %% åŸ·è¡Œæ–‡æœ¬æ¸…æ´—
print("\nğŸ§¹ åŸ·è¡Œæ–‡æœ¬æ¸…æ´—...")
print("é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜...")

df['cleaned_text'] = df[main_text_col].apply(clean_text)
df['processed_text'] = df[main_text_col].apply(
    lambda x: preprocess_text(x, remove_stopwords=True, lemmatize=True)
)

# éæ¿¾ç©ºæ–‡æœ¬
df = df[df['processed_text'].str.len() > 0].reset_index(drop=True)

print(f"âœ… æ¸…æ´—å®Œæˆï¼Œä¿ç•™ {len(df):,} æ¢æœ‰æ•ˆè¨˜éŒ„")

# %% è¨ˆç®—æƒ…æ„Ÿæ¥µæ€§
print("\nğŸ˜Š è¨ˆç®—æƒ…æ„Ÿæ¥µæ€§...")

def get_sentiment_scores(text):
    """ä½¿ç”¨ TextBlob è¨ˆç®—æƒ…æ„Ÿåˆ†æ•¸"""
    try:
        blob = TextBlob(str(text))
        return blob.sentiment.polarity, blob.sentiment.subjectivity
    except:
        return 0, 0

sentiment_scores = df['cleaned_text'].apply(get_sentiment_scores)
df['polarity'] = sentiment_scores.apply(lambda x: x[0])
df['subjectivity'] = sentiment_scores.apply(lambda x: x[1])

# åˆ†é¡æƒ…æ„Ÿ
def classify_sentiment(polarity):
    if polarity > 0.1:
        return 'positive'
    elif polarity < -0.1:
        return 'negative'
    else:
        return 'neutral'

df['sentiment_label'] = df['polarity'].apply(classify_sentiment)

print("âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ")
print("\næƒ…æ„Ÿåˆ†ä½ˆ:")
print(df['sentiment_label'].value_counts())

# %% è¦–è¦ºåŒ–æƒ…æ„Ÿåˆ†ä½ˆ
print("\nğŸ“Š ç”Ÿæˆæƒ…æ„Ÿåˆ†ä½ˆåœ–...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. æƒ…æ„Ÿæ¨™ç±¤åˆ†ä½ˆ
sentiment_counts = df['sentiment_label'].value_counts()
colors = {'positive': 'green', 'neutral': 'gray', 'negative': 'red'}
axes[0, 0].bar(sentiment_counts.index, sentiment_counts.values,
              color=[colors.get(x, 'blue') for x in sentiment_counts.index],
              alpha=0.7, edgecolor='black')
axes[0, 0].set_title('æƒ…æ„Ÿæ¨™ç±¤åˆ†ä½ˆ', fontsize=14, fontweight='bold')
axes[0, 0].set_ylabel('æ•¸é‡')
axes[0, 0].grid(True, alpha=0.3, axis='y')

# 2. æ¥µæ€§åˆ†æ•¸åˆ†ä½ˆ
axes[0, 1].hist(df['polarity'], bins=50, color='skyblue', alpha=0.7, edgecolor='black')
axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2, label='ä¸­æ€§ç·š')
axes[0, 1].set_title('æ¥µæ€§åˆ†æ•¸åˆ†ä½ˆ', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('æ¥µæ€§ (-1: è² é¢, +1: æ­£é¢)')
axes[0, 1].set_ylabel('é »ç‡')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# 3. ä¸»è§€æ€§åˆ†æ•¸åˆ†ä½ˆ
axes[1, 0].hist(df['subjectivity'], bins=50, color='lightcoral', alpha=0.7, edgecolor='black')
axes[1, 0].set_title('ä¸»è§€æ€§åˆ†æ•¸åˆ†ä½ˆ', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('ä¸»è§€æ€§ (0: å®¢è§€, 1: ä¸»è§€)')
axes[1, 0].set_ylabel('é »ç‡')
axes[1, 0].grid(True, alpha=0.3)

# 4. æ¥µæ€§ vs ä¸»è§€æ€§
scatter = axes[1, 1].scatter(df['subjectivity'], df['polarity'], 
                           c=df['sentiment_label'].map(colors),
                           alpha=0.5, s=20, edgecolors='none')
axes[1, 1].axhline(0, color='red', linestyle='--', linewidth=1)
axes[1, 1].set_title('æ¥µæ€§ vs ä¸»è§€æ€§', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('ä¸»è§€æ€§')
axes[1, 1].set_ylabel('æ¥µæ€§')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('visualizations/06_sentiment_analysis.png', dpi=300, bbox_inches='tight')
print("âœ… æƒ…æ„Ÿåˆ†æåœ–å·²ä¿å­˜")
plt.show()

# %% è©é »åˆ†æ
print("\nğŸ“Š è©é »åˆ†æ...")

# æ‰€æœ‰è©
all_words = ' '.join(df['processed_text']).split()
word_freq = Counter(all_words)
top_words = word_freq.most_common(30)

print("\nTop 30 æœ€å¸¸è¦‹è©:")
for word, count in top_words[:10]:
    print(f"  {word:20s}: {count:6,}")

# è¦–è¦ºåŒ– Top è©é »
plt.figure(figsize=(14, 8))
words, counts = zip(*top_words)
plt.barh(range(len(words)), counts, color='steelblue', alpha=0.7)
plt.yticks(range(len(words)), words)
plt.xlabel('é »ç‡')
plt.title('Top 30 æœ€å¸¸è¦‹è©', fontsize=16, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('visualizations/07_word_frequency.png', dpi=300, bbox_inches='tight')
print("âœ… è©é »åœ–å·²ä¿å­˜")
plt.show()

# %% æŒ‰æƒ…æ„Ÿåˆ†æè©é »
print("\nğŸ“Š æŒ‰æƒ…æ„Ÿåˆ†æè©é »...")

for sentiment in ['positive', 'negative']:
    sentiment_text = ' '.join(df[df['sentiment_label'] == sentiment]['processed_text'])
    words = sentiment_text.split()
    word_freq = Counter(words)
    
    print(f"\n{sentiment.upper()} è©•è«– Top 20:")
    for word, count in word_freq.most_common(20)[:10]:
        print(f"  {word:20s}: {count:6,}")

# %% è©é›²
print("\nâ˜ï¸  ç”Ÿæˆè©é›²...")

fig, axes = plt.subplots(1, 3, figsize=(20, 6))

sentiments = ['positive', 'neutral', 'negative']
colors_wc = ['Greens', 'Greys', 'Reds']

for idx, (sentiment, cmap) in enumerate(zip(sentiments, colors_wc)):
    text = ' '.join(df[df['sentiment_label'] == sentiment]['processed_text'])
    
    if text:
        wordcloud = WordCloud(width=800, height=400, 
                            background_color='white',
                            colormap=cmap,
                            max_words=100).generate(text)
        
        axes[idx].imshow(wordcloud, interpolation='bilinear')
        axes[idx].set_title(f'{sentiment.upper()} è©•è«–è©é›²', 
                          fontsize=14, fontweight='bold')
        axes[idx].axis('off')

plt.tight_layout()
plt.savefig('visualizations/08_wordclouds.png', dpi=300, bbox_inches='tight')
print("âœ… è©é›²å·²ä¿å­˜")
plt.show()

# %% TF-IDF ç‰¹å¾µæå–
print("\nğŸ”¤ TF-IDF ç‰¹å¾µæå–...")

tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
tfidf_matrix = tfidf.fit_transform(df['processed_text'])

print(f"âœ… TF-IDF çŸ©é™£å½¢ç‹€: {tfidf_matrix.shape}")

# æå– top TF-IDF è©
feature_names = tfidf.get_feature_names_out()
dense = tfidf_matrix.todense()
denselist = dense.tolist()
tfidf_df = pd.DataFrame(denselist, columns=feature_names)

# Top TF-IDF ç‰¹å¾µ
top_tfidf = tfidf_df.mean().sort_values(ascending=False).head(30)

plt.figure(figsize=(14, 8))
top_tfidf.plot(kind='barh', color='darkgreen', alpha=0.7)
plt.xlabel('å¹³å‡ TF-IDF åˆ†æ•¸')
plt.title('Top 30 TF-IDF ç‰¹å¾µ', fontsize=16, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('visualizations/09_tfidf_features.png', dpi=300, bbox_inches='tight')
print("âœ… TF-IDF ç‰¹å¾µåœ–å·²ä¿å­˜")
plt.show()

# %% N-gram åˆ†æ
print("\nğŸ“ N-gram åˆ†æ...")

# Bigrams
vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=30)
bigrams = vectorizer.fit_transform(df['processed_text'])
bigram_freq = dict(zip(vectorizer.get_feature_names_out(), 
                       bigrams.sum(axis=0).tolist()[0]))
bigram_freq = dict(sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True))

print("\nTop 20 Bigrams:")
for bigram, count in list(bigram_freq.items())[:20]:
    print(f"  {bigram:30s}: {count:6,}")

# è¦–è¦ºåŒ–
plt.figure(figsize=(14, 8))
bigrams_list = list(bigram_freq.items())[:25]
phrases, counts = zip(*bigrams_list)
plt.barh(range(len(phrases)), counts, color='purple', alpha=0.7)
plt.yticks(range(len(phrases)), phrases)
plt.xlabel('é »ç‡')
plt.title('Top 25 Bigrams (å…©è©çµ„åˆ)', fontsize=16, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('visualizations/10_bigrams.png', dpi=300, bbox_inches='tight')
print("âœ… Bigram åœ–å·²ä¿å­˜")
plt.show()

# %% ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š
print("\nğŸ’¾ ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š...")

# ä¿å­˜å®Œæ•´æ•¸æ“š
processed_path = Path('data/processed/preprocessed_data.csv')
df.to_csv(processed_path, index=False)
print(f"âœ… é è™•ç†æ•¸æ“šå·²ä¿å­˜: {processed_path}")

# ä¿å­˜ TF-IDF ç‰¹å¾µ
import pickle
tfidf_path = Path('data/processed/tfidf_vectorizer.pkl')
with open(tfidf_path, 'wb') as f:
    pickle.dump(tfidf, f)
print(f"âœ… TF-IDF å‘é‡å™¨å·²ä¿å­˜: {tfidf_path}")

# ä¿å­˜ç‰¹å¾µçŸ©é™£
from scipy.sparse import save_npz
features_path = Path('data/processed/tfidf_features.npz')
save_npz(features_path, tfidf_matrix)
print(f"âœ… TF-IDF ç‰¹å¾µçŸ©é™£å·²ä¿å­˜: {features_path}")

# %% ç”Ÿæˆé è™•ç†å ±å‘Š
print("\nğŸ“„ ç”Ÿæˆé è™•ç†å ±å‘Š...")

report_path = Path('data/processed/preprocessing_report.txt')
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("=" * 80 + "\n")
    f.write("æ–‡æœ¬é è™•ç†å ±å‘Š\n")
    f.write("=" * 80 + "\n\n")
    
    f.write(f"è™•ç†è¨˜éŒ„æ•¸: {len(df):,}\n")
    f.write(f"ä¸»è¦æ–‡æœ¬æ¬„ä½: {main_text_col}\n\n")
    
    f.write("æƒ…æ„Ÿåˆ†ä½ˆ:\n")
    f.write(str(df['sentiment_label'].value_counts()))
    f.write("\n\n")
    
    f.write("æ¥µæ€§çµ±è¨ˆ:\n")
    f.write(str(df['polarity'].describe()))
    f.write("\n\n")
    
    f.write("Top 30 è©é »:\n")
    for word, count in top_words:
        f.write(f"  {word:30s}: {count:8,}\n")
    
    f.write("\n\nTF-IDF ç‰¹å¾µæ•¸: 1000\n")
    f.write(f"ç‰¹å¾µçŸ©é™£å½¢ç‹€: {tfidf_matrix.shape}\n")

print(f"âœ… é è™•ç†å ±å‘Šå·²ä¿å­˜: {report_path}")

# %% ç¸½çµ
print("\n" + "=" * 80)
print("ğŸ‰ æ–‡æœ¬é è™•ç†å®Œæˆ")
print("=" * 80)
print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
print(f"  ğŸ“Š è¦–è¦ºåŒ–: visualizations/ (6 å¼µåœ–)")
print(f"  ğŸ’¾ é è™•ç†æ•¸æ“š: {processed_path}")
print(f"  ğŸ”¤ TF-IDF å‘é‡å™¨: {tfidf_path}")
print(f"  ğŸ“Š ç‰¹å¾µçŸ©é™£: {features_path}")
print(f"  ğŸ“„ å ±å‘Š: {report_path}")
print("\nä¸‹ä¸€æ­¥: æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°")
print("=" * 80)
