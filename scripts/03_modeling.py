"""
æ‰‹æ©Ÿè©•è«–æƒ…æ„Ÿåˆ†æ - æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°
Model Training and Evaluation
"""

# %% å°å…¥å¥—ä»¶
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pickle
from scipy.sparse import load_npz

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import (classification_report, confusion_matrix, 
                            accuracy_score, precision_recall_fscore_support,
                            roc_curve, auc, roc_auc_score)
from sklearn.preprocessing import label_binarize
import warnings
warnings.filterwarnings('ignore')

# è¨­å®š
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 8)
Path('visualizations').mkdir(exist_ok=True)
Path('models').mkdir(exist_ok=True)

print("=" * 80)
print("æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°")
print("=" * 80)

# %% è¼‰å…¥æ•¸æ“š
print("\nğŸ“‚ è¼‰å…¥æ•¸æ“š...")

df = pd.read_csv('data/processed/preprocessed_data.csv')
X = load_npz('data/processed/tfidf_features.npz')
y = df['sentiment_label']

print(f"âœ… ç‰¹å¾µçŸ©é™£: {X.shape}")
print(f"âœ… æ¨™ç±¤åˆ†ä½ˆ:\n{y.value_counts()}")

# %% è³‡æ–™åˆ†å‰²
print("\nâœ‚ï¸  åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†...")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"âœ… è¨“ç·´é›†: {X_train.shape[0]:,} æ¨£æœ¬")
print(f"âœ… æ¸¬è©¦é›†: {X_test.shape[0]:,} æ¨£æœ¬")

# %% å®šç¾©æ¨¡å‹
print("\nğŸ¤– å®šç¾©æ¨¡å‹...")

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Naive Bayes': MultinomialNB(),
    'Linear SVM': LinearSVC(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

print(f"âœ… å®šç¾©äº† {len(models)} å€‹æ¨¡å‹")

# %% è¨“ç·´èˆ‡è©•ä¼°æ‰€æœ‰æ¨¡å‹
print("\nğŸ‹ï¸  è¨“ç·´æ¨¡å‹...")

results = {}

for name, model in models.items():
    print(f"\n{'='*60}")
    print(f"è¨“ç·´: {name}")
    print('='*60)
    
    # è¨“ç·´
    model.fit(X_train, y_train)
    
    # é æ¸¬
    y_pred = model.predict(X_test)
    
    # è©•ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_test, y_pred, average='weighted', zero_division=0
    )
    
    # äº¤å‰é©—è­‰ï¼ˆé¸æ“‡éƒ¨åˆ†æ¨¡å‹ä»¥ç¯€çœæ™‚é–“ï¼‰
    if name in ['Logistic Regression', 'Naive Bayes']:
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()
    else:
        cv_mean, cv_std = None, None
    
    # ä¿å­˜çµæœ
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'cv_mean': cv_mean,
        'cv_std': cv_std,
        'y_pred': y_pred
    }
    
    print(f"æº–ç¢ºç‡: {accuracy:.4f}")
    print(f"ç²¾ç¢ºç‡: {precision:.4f}")
    print(f"å¬å›ç‡: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    if cv_mean is not None:
        print(f"äº¤å‰é©—è­‰: {cv_mean:.4f} (+/- {cv_std:.4f})")
    
    # è©³ç´°åˆ†é¡å ±å‘Š
    print("\nåˆ†é¡å ±å‘Š:")
    print(classification_report(y_test, y_pred, zero_division=0))

# %% æ¨¡å‹æ¯”è¼ƒ
print("\n" + "=" * 80)
print("ğŸ“Š æ¨¡å‹æ¯”è¼ƒ")
print("=" * 80)

comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [r['accuracy'] for r in results.values()],
    'Precision': [r['precision'] for r in results.values()],
    'Recall': [r['recall'] for r in results.values()],
    'F1-Score': [r['f1'] for r in results.values()]
})

comparison_df = comparison_df.sort_values('Accuracy', ascending=False)
print(comparison_df.to_string(index=False))

# è¦–è¦ºåŒ–æ¨¡å‹æ¯”è¼ƒ
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
for idx, metric in enumerate(metrics):
    ax = axes[idx // 2, idx % 2]
    comparison_df.plot(x='Model', y=metric, kind='bar', ax=ax, 
                      color='steelblue', alpha=0.7, legend=False)
    ax.set_title(f'{metric} æ¯”è¼ƒ', fontsize=14, fontweight='bold')
    ax.set_ylabel(metric)
    ax.set_xlabel('')
    ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')
    ax.grid(True, alpha=0.3, axis='y')
    ax.set_ylim([0, 1])
    
    # é¡¯ç¤ºæ•¸å€¼
    for i, v in enumerate(comparison_df[metric]):
        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.savefig('visualizations/11_model_comparison.png', dpi=300, bbox_inches='tight')
print("\nâœ… æ¨¡å‹æ¯”è¼ƒåœ–å·²ä¿å­˜")
plt.show()

# %% æ‰¾å‡ºæœ€ä½³æ¨¡å‹
best_model_name = comparison_df.iloc[0]['Model']
best_model = results[best_model_name]['model']
best_y_pred = results[best_model_name]['y_pred']

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"æº–ç¢ºç‡: {results[best_model_name]['accuracy']:.4f}")

# %% æ··æ·†çŸ©é™£
print("\nğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é™£...")

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

for idx, (name, result) in enumerate(results.items()):
    if idx >= 6:
        break
    
    cm = confusion_matrix(y_test, result['y_pred'])
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
               xticklabels=sorted(y.unique()),
               yticklabels=sorted(y.unique()))
    axes[idx].set_title(f'{name}\nAccuracy: {result["accuracy"]:.3f}', 
                       fontsize=12, fontweight='bold')
    axes[idx].set_ylabel('çœŸå¯¦æ¨™ç±¤')
    axes[idx].set_xlabel('é æ¸¬æ¨™ç±¤')

plt.tight_layout()
plt.savefig('visualizations/12_confusion_matrices.png', dpi=300, bbox_inches='tight')
print("âœ… æ··æ·†çŸ©é™£å·²ä¿å­˜")
plt.show()

# %% æœ€ä½³æ¨¡å‹è©³ç´°åˆ†æ
print(f"\nğŸ” {best_model_name} è©³ç´°åˆ†æ...")

# æ··æ·†çŸ©é™£
cm = confusion_matrix(y_test, best_y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', 
           xticklabels=sorted(y.unique()),
           yticklabels=sorted(y.unique()),
           cbar_kws={'label': 'æ•¸é‡'})
plt.title(f'{best_model_name} - æ··æ·†çŸ©é™£', fontsize=16, fontweight='bold')
plt.ylabel('çœŸå¯¦æ¨™ç±¤', fontsize=12)
plt.xlabel('é æ¸¬æ¨™ç±¤', fontsize=12)
plt.tight_layout()
plt.savefig('visualizations/13_best_model_confusion_matrix.png', dpi=300, bbox_inches='tight')
print("âœ… æœ€ä½³æ¨¡å‹æ··æ·†çŸ©é™£å·²ä¿å­˜")
plt.show()

# %% ç‰¹å¾µé‡è¦æ€§åˆ†æï¼ˆå¦‚æœæ¨¡å‹æ”¯æ´ï¼‰
if hasattr(best_model, 'coef_'):
    print("\nğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ...")
    
    # è¼‰å…¥ TF-IDF å‘é‡å™¨
    with open('data/processed/tfidf_vectorizer.pkl', 'rb') as f:
        tfidf = pickle.load(f)
    
    feature_names = tfidf.get_feature_names_out()
    
    # ç²å–ä¿‚æ•¸ï¼ˆå¤šé¡åˆ¥ï¼‰
    if len(best_model.coef_.shape) > 1:
        # å–å¹³å‡æˆ–é¸æ“‡ç‰¹å®šé¡åˆ¥
        coef = np.abs(best_model.coef_).mean(axis=0)
    else:
        coef = best_model.coef_
    
    # Top ç‰¹å¾µ
    top_n = 30
    top_indices = np.argsort(coef)[-top_n:]
    top_features = [feature_names[i] for i in top_indices]
    top_values = [coef[i] for i in top_indices]
    
    plt.figure(figsize=(12, 10))
    plt.barh(range(len(top_features)), top_values, color='darkgreen', alpha=0.7)
    plt.yticks(range(len(top_features)), top_features)
    plt.xlabel('é‡è¦æ€§')
    plt.title(f'{best_model_name} - Top {top_n} é‡è¦ç‰¹å¾µ', 
             fontsize=16, fontweight='bold')
    plt.grid(True, alpha=0.3, axis='x')
    plt.tight_layout()
    plt.savefig('visualizations/14_feature_importance.png', dpi=300, bbox_inches='tight')
    print("âœ… ç‰¹å¾µé‡è¦æ€§åœ–å·²ä¿å­˜")
    plt.show()

# %% éŒ¯èª¤åˆ†æ
print("\nğŸ” éŒ¯èª¤åˆ†æ...")

# æ‰¾å‡ºé æ¸¬éŒ¯èª¤çš„æ¨£æœ¬
errors = y_test != best_y_pred
error_indices = np.where(errors)[0]

print(f"éŒ¯èª¤é æ¸¬æ•¸: {errors.sum():,} ({errors.sum()/len(y_test)*100:.2f}%)")

# éŒ¯èª¤é¡å‹åˆ†æ
error_analysis = pd.DataFrame({
    'True': y_test.values[errors],
    'Predicted': best_y_pred[errors]
})
error_counts = error_analysis.groupby(['True', 'Predicted']).size().reset_index(name='Count')
error_counts = error_counts.sort_values('Count', ascending=False)

print("\néŒ¯èª¤é¡å‹åˆ†ä½ˆ:")
print(error_counts.to_string(index=False))

# %% ROC æ›²ç·šï¼ˆå¤šé¡åˆ¥ï¼‰
if len(np.unique(y)) <= 3:  # åªå° 3 é¡æˆ–æ›´å°‘ç¹ªè£½ ROC
    print("\nğŸ“ˆ ç”Ÿæˆ ROC æ›²ç·š...")
    
    # äºŒå€¼åŒ–æ¨™ç±¤
    y_test_bin = label_binarize(y_test, classes=sorted(y.unique()))
    n_classes = y_test_bin.shape[1]
    
    # ç²å–é æ¸¬æ¦‚ç‡
    if hasattr(best_model, 'predict_proba'):
        y_score = best_model.predict_proba(X_test)
    elif hasattr(best_model, 'decision_function'):
        y_score = best_model.decision_function(X_test)
        # æ¨™æº–åŒ–åˆ° [0, 1]
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        y_score = scaler.fit_transform(y_score)
    else:
        print("âš ï¸  è©²æ¨¡å‹ä¸æ”¯æ´æ¦‚ç‡é æ¸¬ï¼Œè·³é ROC æ›²ç·š")
        y_score = None
    
    if y_score is not None:
        plt.figure(figsize=(10, 8))
        
        colors = ['red', 'green', 'blue']
        for i, (color, class_name) in enumerate(zip(colors, sorted(y.unique()))):
            if i < n_classes:
                fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
                roc_auc = auc(fpr, tpr)
                plt.plot(fpr, tpr, color=color, lw=2, 
                        label=f'{class_name} (AUC = {roc_auc:.3f})')
        
        plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title(f'{best_model_name} - ROC æ›²ç·š', fontsize=16, fontweight='bold')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('visualizations/15_roc_curves.png', dpi=300, bbox_inches='tight')
        print("âœ… ROC æ›²ç·šå·²ä¿å­˜")
        plt.show()

# %% ä¿å­˜æœ€ä½³æ¨¡å‹
print("\nğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹...")

model_path = Path(f'models/best_model_{best_model_name.replace(" ", "_")}.pkl')
with open(model_path, 'wb') as f:
    pickle.dump(best_model, f)

print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_path}")

# ä¿å­˜æ‰€æœ‰æ¨¡å‹çµæœ
results_path = Path('models/all_results.pkl')
with open(results_path, 'wb') as f:
    pickle.dump(results, f)
print(f"âœ… æ‰€æœ‰çµæœå·²ä¿å­˜: {results_path}")

# %% ç”Ÿæˆæ¨¡å‹å ±å‘Š
print("\nğŸ“„ ç”Ÿæˆæ¨¡å‹å ±å‘Š...")

report_path = Path('models/model_evaluation_report.txt')
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("=" * 80 + "\n")
    f.write("æ¨¡å‹è©•ä¼°å ±å‘Š\n")
    f.write("=" * 80 + "\n\n")
    
    f.write(f"æ•¸æ“šé›†å¤§å°:\n")
    f.write(f"  è¨“ç·´é›†: {X_train.shape[0]:,} æ¨£æœ¬\n")
    f.write(f"  æ¸¬è©¦é›†: {X_test.shape[0]:,} æ¨£æœ¬\n")
    f.write(f"  ç‰¹å¾µæ•¸: {X.shape[1]:,}\n\n")
    
    f.write("æ¨¡å‹æ¯”è¼ƒ:\n")
    f.write("-" * 80 + "\n")
    f.write(comparison_df.to_string(index=False))
    f.write("\n\n")
    
    f.write(f"æœ€ä½³æ¨¡å‹: {best_model_name}\n")
    f.write("-" * 80 + "\n")
    f.write(f"æº–ç¢ºç‡: {results[best_model_name]['accuracy']:.4f}\n")
    f.write(f"ç²¾ç¢ºç‡: {results[best_model_name]['precision']:.4f}\n")
    f.write(f"å¬å›ç‡: {results[best_model_name]['recall']:.4f}\n")
    f.write(f"F1-Score: {results[best_model_name]['f1']:.4f}\n\n")
    
    f.write("è©³ç´°åˆ†é¡å ±å‘Š:\n")
    f.write("-" * 80 + "\n")
    f.write(classification_report(y_test, best_y_pred, zero_division=0))
    f.write("\n\n")
    
    f.write("éŒ¯èª¤åˆ†æ:\n")
    f.write("-" * 80 + "\n")
    f.write(f"éŒ¯èª¤é æ¸¬æ•¸: {errors.sum():,} ({errors.sum()/len(y_test)*100:.2f}%)\n\n")
    f.write("éŒ¯èª¤é¡å‹:\n")
    f.write(error_counts.to_string(index=False))

print(f"âœ… æ¨¡å‹å ±å‘Šå·²ä¿å­˜: {report_path}")

# %% ç¸½çµ
print("\n" + "=" * 80)
print("ğŸ‰ æ¨¡å‹è¨“ç·´èˆ‡è©•ä¼°å®Œæˆ")
print("=" * 80)
print(f"\næœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"æº–ç¢ºç‡: {results[best_model_name]['accuracy']:.4f}")
print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
print(f"  ğŸ“Š è¦–è¦ºåŒ–: visualizations/ (5 å¼µåœ–)")
print(f"  ğŸ¤– æœ€ä½³æ¨¡å‹: {model_path}")
print(f"  ğŸ’¾ æ‰€æœ‰çµæœ: {results_path}")
print(f"  ğŸ“„ è©•ä¼°å ±å‘Š: {report_path}")
print("\nå®Œæˆï¼å¯ä»¥ä½¿ç”¨æœ€ä½³æ¨¡å‹é€²è¡Œé æ¸¬")
print("=" * 80)
