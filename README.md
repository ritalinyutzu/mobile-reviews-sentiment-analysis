Mobile Reviews Sentiment Analysis

åˆ©ç”¨æ©Ÿå™¨å­¸ç¿’ï¼NLP å°æ‰‹æ©Ÿå•†å“è©•è«– (mobile product reviews) åšæƒ…æ„Ÿåˆ†æã€‚
ç›®æ¨™æ˜¯ï¼šä¸€å¤§åŒ…æ–‡å­—è©•è«– â†’ åˆ¤æ–·å®ƒæ˜¯æ­£å‘ã€è² å‘ï¼Œç„¶å¾Œç”¢ä¸€ä»½ã€Œæ•¸æ“šå ±å‘Šï¼‹åœ–è¡¨ã€ã€‚
é€™å€‹ repo å¯ä»¥æ‹¿å»ç•¶ Demo / ç·šä¸Šå±•ç¤ºé  / é¢è©¦ä½œå“é›†ï¼Œä¸€æ­¥ä¸€æ­¥ç…§åšå°±èƒ½è·‘ã€‚

Dataset: ä»¥ã€Œç´„ 50K ç­†æ‰‹æ©Ÿç›¸é—œè©•è«–ã€ç‚ºå‡è¨­è¦æ¨¡ï¼Œå¯¦æ¸¬å¯é” 85%+ accuracyï¼ˆè¦–ä½ é¸çš„æ¨¡å‹ã€æ¸…æ´—è¦å‰‡è€Œå®šï¼‰ã€‚

1. å°ˆæ¡ˆç›®æ¨™ (What this repo does)

ä¸€ã€æŠŠåŸå§‹çš„æ‰‹æ©Ÿè©•è«–è³‡æ–™ï¼ˆcsv / json / excelï¼‰æ¸…æ´—æˆ NLP å¯ä»¥åƒçš„æ–‡å­—ã€‚
äºŒã€æŠŠæ–‡å­—è½‰æˆç‰¹å¾µï¼ˆBag-of-Wordsã€TF-IDFã€n-gramï¼‰ã€‚
ä¸‰ã€è¨“ç·´ä¸€å€‹æˆ–å¤šå€‹æƒ…æ„Ÿåˆ†é¡æ¨¡å‹ï¼ˆLogistic Regression / SVM / Random Forest / XGBoostâ€¦ï¼‰ã€‚
å››ã€è¼¸å‡ºè©•ä¼°æŒ‡æ¨™ï¼ˆaccuracy, precision, recall, f1, confusion matrixï¼‰ã€‚
äº”ã€æŠŠåˆ†æçµæœç•«åœ–ï¼Œå­˜åœ¨ visualizations/ï¼Œæ–¹ä¾¿ä½ åšç¶²é  demoã€‚
å…­ã€ä¿ç•™ä¸€å€‹ã€Œä¹‹å¾Œè¦æ›æ·±åº¦å­¸ç¿’ / BERT / HuggingFaceã€çš„å…¥å£ã€‚

mobile-reviews-sentiment-analysis/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # ä½ æ‹¿åˆ°çš„åŸå§‹è³‡æ–™æ”¾é€™è£¡ (e.g. mobile_reviews_raw.csv)
â”‚   â”œâ”€â”€ processed/            # å‰è™•ç†å¾Œçš„ä¹¾æ·¨è³‡æ–™ (e.g. mobile_reviews_clean.csv)
â”‚   â””â”€â”€ README.md             # (å¯é¸) èªªæ˜è³‡æ–™ä¾†æºã€æ¬„ä½æ„ç¾©
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_preprocess.py      # å‰è™•ç† / æ¸…æ´—è³‡æ–™
â”‚   â”œâ”€â”€ 02_feature_engineering.py  # TF-IDFã€n-gramã€Embedding
â”‚   â”œâ”€â”€ 03_train_model.py     # è¨“ç·´æ¨¡å‹ï¼Œæœƒå­˜åˆ° models/
â”‚   â”œâ”€â”€ 04_evaluate.py        # è©•ä¼°ã€ç”¢ç”Ÿå ±è¡¨ã€æ··æ·†çŸ©é™£
â”‚   â””â”€â”€ 05_inference.py       # çµ¦æ–°è©•è«–å°±èƒ½é æ¸¬ (demo ç”¨)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl  # ç‰¹å¾µè½‰æ›å™¨
â”‚   â”œâ”€â”€ sentiment_model.pkl   # è¨“ç·´å¥½çš„æ¨¡å‹ (e.g. LogisticRegression)
â”‚   â””â”€â”€ label_encoder.pkl     # æŠŠ positive / negative è½‰æˆ 1 / 0
â”‚
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ sentiment_distribution.png   # æ­£è² å‘è©•è«–æ¯”ä¾‹
â”‚   â”œâ”€â”€ confusion_matrix.png         # æ··æ·†çŸ©é™£
â”‚   â”œâ”€â”€ top_words_positive.png       # æ­£å‘è©•è«–å¸¸è¦‹è©
â”‚   â””â”€â”€ top_words_negative.png       # è² å‘è©•è«–å¸¸è¦‹è©
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md   â† å°±æ˜¯é€™å€‹

3. æµç¨‹åœ– (Data & ML pipeline)
flowchart TD
    A[raw reviews CSV in data/raw] --> B[01_preprocess.py<br/>æ¸…æ´—ã€å»é™¤HTMLã€è½‰å°å¯«ã€å»åœç”¨è©]
    B --> C[02_feature_engineering.py<br/>TF-IDF / n-gram / tokenize]
    C --> D[03_train_model.py<br/>train/test split, fit model, save .pkl]
    D --> E[04_evaluate.py<br/>classification report, confusion matrix, export to visualizations/]
    E --> F[05_inference.py<br/>web/demo ç”¨çš„å³æ™‚é æ¸¬]

4. å®‰è£èˆ‡åŸ·è¡Œ (Setup & Run in VS Code)
ä¸€ã€clone å°ˆæ¡ˆ
git clone https://github.com/ritalinyutzu/mobile-reviews-sentiment-analysis.git
cd mobile-reviews-sentiment-analysis

äºŒã€å»ºç«‹è™›æ“¬ç’°å¢ƒï¼ˆå»ºè­°ï¼‰
python -m venv venv
source venv/bin/activate      # Windows: venv\Scripts\activate

ä¸‰ã€å®‰è£å¥—ä»¶
pip install -r requirements.txt

å››ã€æŠŠåŸå§‹è³‡æ–™æ”¾é€²å»
æª”æ¡ˆæ”¾åœ¨ï¼šdata/raw/mobile_reviews_raw.csv
è‡³å°‘è¦æœ‰é€™å¹¾æ¬„ï¼š
review_textï¼šè©•è«–å…§å®¹
ratingï¼š1~5 æ˜Ÿ
ï¼ˆå¯é¸ï¼‰reviewer_id, app_name, device â€¦
å¦‚æœä½ æ²’æœ‰ labelï¼Œå¯ä»¥è¦å‰‡æ¨™ï¼šrating â‰¥ 4 â†’ positiveï¼›rating â‰¤ 2 â†’ negativeï¼›rating=3 â†’ ä¸Ÿæ‰æˆ–æ­¸ä¸­æ€§ã€‚

äº”ã€ä¾åºè·‘è…³æœ¬
python scripts/01_preprocess.py
python scripts/02_feature_engineering.py
python scripts/03_train_model.py
python scripts/04_evaluate.py

å…­ã€è¦åš demo / å³æ™‚é æ¸¬
python scripts/05_inference.py --text "The battery is terrible but the screen is good."

5. å‰è™•ç† (Preprocessing)
è¼¸å…¥ï¼šdata/raw/mobile_reviews_raw.csv
è¼¸å‡ºï¼šdata/processed/mobile_reviews_clean.csv

å‰è™•ç†æœƒåšé€™å¹¾ä»¶äº‹ï¼ˆç…§é †åºï¼‰ï¼š

ä¸€ã€è¼‰å…¥è³‡æ–™
ç”¨ pandas.read_csv()
æª¢æŸ¥ç¼ºå¤±å€¼ â†’ è‹¥ review_text ç‚ºç©ºï¼Œç›´æ¥ä¸Ÿæ‰
è‹¥æœ‰é‡è¤‡è©•è«–ï¼ˆåŒä¸€å€‹äººã€åŒä¸€æ®µè©±ï¼‰ï¼Œå¯ä»¥ç”¨ drop_duplicates(subset=['review_text'])

äºŒã€æ–‡å­—æ­£è¦åŒ–
å…¨éƒ¨è½‰å°å¯«
ç§»é™¤ HTML tagï¼šBeautifulSoup æˆ– regex (re.sub(r'<.*?>', '', text))
ç§»é™¤ç¶²å€ã€emailã€@å¸³è™Ÿï¼šre.sub(r'http\S+|www.\S+', '', text)
ç§»é™¤å¤šé¤˜ç©ºç™½ï¼š" ".join(text.split())

ä¸‰ã€æ¨™é»èˆ‡è¡¨æƒ…ç¬¦è™Ÿè™•ç†
å¦‚æœæ˜¯è‹±æ–‡è©•è«–ï¼šå¯ä»¥ç§»é™¤æ¨™é»
å¦‚æœä½ æœ‰ emoji è¦è¾¨è­˜æˆæƒ…æ„Ÿï¼ˆğŸ™‚ ğŸ‘ ğŸ˜¡ï¼‰ï¼Œå¯ä»¥å…ˆæ›¿æ›æˆæ–‡å­—ï¼Œä¾‹å¦‚ :smile:, :angry:

å››ã€åœç”¨è© (stopwords)
ä½¿ç”¨ nltk.corpus.stopwords or sklearn.feature_extraction.text.ENGLISH_STOP_WORDS
ç§»é™¤ like, the, a, it, toâ€¦ é€™ç¨®æ²’æœ‰æƒ…æ„Ÿçš„è©

äº”ã€è©å½¢é‚„åŸ (lemmatization/stemming)
å¯ä»¥ç”¨ nltk.WordNetLemmatizer
æŠŠ loved, loving â†’ love

å…­ã€ç”¢ sentiment label
å¦‚æœä½ çš„åŸå§‹è³‡æ–™æœ‰æ˜Ÿç­‰ï¼Œå°±åšï¼š
def map_rating_to_label(r):
    if r >= 4:
        return "positive"
    elif r <= 2:
        return "negative"
    else:
        return "neutral"
å¦‚æœä½ ä¸æƒ³è¦ neutralï¼Œå°±æŠŠå®ƒä¸Ÿæ‰ï¼š
df = df[df['sentiment'] != 'neutral']

ä¸ƒã€å­˜æª”
å­˜æˆï¼šdata/processed/mobile_reviews_clean.csv
é€™å€‹æª”æ¡ˆæ˜¯å¾ŒçºŒæ‰€æœ‰æ¨¡å‹çš„ã€Œä¹¾æ·¨ç‰ˆè³‡æ–™ã€
ğŸ“„ ç¯„ä¾‹ç¨‹å¼ï¼ˆ01_preprocess.pyï¼‰ï¼š
import pandas as pd
import re
from pathlib import Path

RAW_PATH = Path("data/raw/mobile_reviews_raw.csv")
OUT_PATH = Path("data/processed/mobile_reviews_clean.csv")
OUT_PATH.parent.mkdir(parents=True, exist_ok=True)

def clean_text(text: str) -> str:
    text = str(text).lower()
    text = re.sub(r'<.*?>', ' ', text)           # remove HTML
    text = re.sub(r'http\S+|www.\S+', ' ', text) # remove urls
    text = re.sub(r'[^a-z0-9\s]', ' ', text)     # keep only letters/digits
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df = pd.read_csv(RAW_PATH)
df = df.dropna(subset=['review_text']).drop_duplicates(subset=['review_text'])

df['clean_text'] = df['review_text'].apply(clean_text)

def map_rating_to_label(r):
    if r >= 4: return "positive"
    elif r <= 2: return "negative"
    else: return "neutral"

df['sentiment'] = df['rating'].apply(map_rating_to_label)
df = df[df['sentiment'] != 'neutral']   # optional

df.to_csv(OUT_PATH, index=False)
print(f"âœ… cleaned data saved to {OUT_PATH} with shape={df.shape}")

6. ç‰¹å¾µå·¥ç¨‹ (Feature Engineering)
é€™ä¸€æ­¥æ˜¯æŠŠæ–‡å­—è®Šæˆæ©Ÿå™¨å­¸ç¿’èƒ½åƒçš„æ•¸å­—ã€‚
ä¸€ã€è¼‰å…¥å‰›å‰›çš„ä¹¾æ·¨è³‡æ–™
äºŒã€åˆ‡ train / testï¼ˆe.g. 80/20ï¼‰
ä¸‰ã€å»ºç«‹ TF-IDF vectorizer
max_features=20000
ngram_range=(1,2) â†’ å¯ä»¥æŠ“åˆ°ã€Œbattery lifeã€ã€Œfast chargingã€é€™ç¨®ç‰‡èª
min_df=2 â†’ å¤ªå°‘è¦‹çš„è©ä¸Ÿæ‰
å››ã€æŠŠ vectorizer å­˜èµ·ä¾†ï¼ˆä¹‹å¾Œé æ¸¬è¦ç”¨ä¸€æ¨£çš„ï¼‰
ğŸ“„ ç¯„ä¾‹ï¼ˆ02_feature_engineering.pyï¼‰ï¼š
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib
from pathlib import Path

DATA_PATH = Path("data/processed/mobile_reviews_clean.csv")
VECT_PATH = Path("models/tfidf_vectorizer.pkl")
VECT_PATH.parent.mkdir(parents=True, exist_ok=True)

df = pd.read_csv(DATA_PATH)
X = df['clean_text'].values
y = df['sentiment'].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

tfidf = TfidfVectorizer(
    max_features=20000,
    ngram_range=(1,2),
    stop_words='english'
)
tfidf.fit(X_train)

joblib.dump({
    "vectorizer": tfidf,
    "X_train": X_train,
    "X_test": X_test,
    "y_train": y_train,
    "y_test": y_test
}, VECT_PATH)

print("âœ… TF-IDF fitted and saved to", VECT_PATH)

7. å»ºæ¨¡ (Modeling)
é€™è£¡å…ˆç”¨æœ€ç©©ã€æœ€ä¸æœƒå‡ºåŒ…ã€ä¹Ÿæœ€å®¹æ˜“å¯«åœ¨ README çš„ï¼šLogistic Regressionã€‚
ä½ ä¹Ÿå¯ä»¥å¹³è¡Œè©¦ï¼šSVM, Linear SVC, RandomForest, XGBoostã€‚
å¦‚æœä½ è¦åšæˆç¶²é  demoï¼ŒLogReg / LinearSVC è¼‰å…¥é€Ÿåº¦æœ€å¿«ã€‚
ğŸ“„ ç¯„ä¾‹ï¼ˆ03_train_model.pyï¼‰ï¼š
import joblib
from pathlib import Path
from sklearn.linear_model import LogisticRegression

VECT_PATH = Path("models/tfidf_vectorizer.pkl")
MODEL_PATH = Path("models/sentiment_model.pkl")

bundle = joblib.load(VECT_PATH)
tfidf = bundle["vectorizer"]
X_train = tfidf.transform(bundle["X_train"])
y_train = bundle["y_train"]

clf = LogisticRegression(
    max_iter=300,
    n_jobs=-1
)
clf.fit(X_train, y_train)

joblib.dump({
    "model": clf,
    "vectorizer": tfidf
}, MODEL_PATH)

print("âœ… Model trained and saved to", MODEL_PATH)

8. è©•ä¼°èˆ‡è¦–è¦ºåŒ– (Evaluation & Visualizations)

é€™æ­¥å°±æ˜¯ä½ èªªçš„ã€ŒæŒ‘å¹¾å¼µåœ–å‡ºä¾†ï¼Œè¦çµ¦ç¶²é ç”¨ã€ã€‚

ä¸€ã€è¼‰å…¥å‰›å‰›çš„æ¨¡å‹ï¼‹TF-IDF
äºŒã€è·‘ test set â†’ ç”¢ç”Ÿ
accuracy
precision, recall, f1
classification_report
confusion matrix

ä¸‰ã€ç•«åœ–
visualizations/sentiment_distribution.png â†’ è¨ˆç®— positive / negative æ¯”ä¾‹
visualizations/confusion_matrix.png â†’ ç”¨ seaborn / matplotlib ç•«
visualizations/top_words_positive.png / visualizations/top_words_negative.png â†’ å¾ TF-IDF è£¡æŠ“æœ€é«˜æ¬Šé‡è©
ğŸ“„ ç¯„ä¾‹ï¼ˆ04_evaluate.pyï¼‰ï¼š
import joblib
import pandas as pd
from pathlib import Path
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

MODEL_PATH = Path("models/sentiment_model.pkl")
DATA_PATH = Path("data/processed/mobile_reviews_clean.csv")
VIZ_DIR = Path("visualizations")
VIZ_DIR.mkdir(parents=True, exist_ok=True)

bundle = joblib.load(MODEL_PATH)
model = bundle["model"]
vectorizer = bundle["vectorizer"]

df = pd.read_csv(DATA_PATH)
X = df['clean_text'].values
y = df['sentiment'].values

# é€™è£¡ä¹Ÿå¯ä»¥åªç”¨ testï¼Œä¸éæˆ‘é€™è£¡å…¨è·‘ä¸€æ¬¡çµ¦ä½ çœ‹
X_vec = vectorizer.transform(X)
y_pred = model.predict(X_vec)

print("Accuracy:", accuracy_score(y, y_pred))
print(classification_report(y, y_pred))

# 1) æ··æ·†çŸ©é™£
cm = confusion_matrix(y, y_pred, labels=['negative','positive'])
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['neg','pos'],
            yticklabels=['neg','pos'])
plt.title("Confusion Matrix")
plt.savefig(VIZ_DIR / "confusion_matrix.png", bbox_inches='tight')

# 2) æƒ…æ„Ÿåˆ†å¸ƒ
sent_counts = df['sentiment'].value_counts()
plt.figure(figsize=(5,4))
sent_counts.plot(kind='bar')
plt.title("Sentiment Distribution")
plt.ylabel("Count")
plt.savefig(VIZ_DIR / "sentiment_distribution.png", bbox_inches='tight')

print("âœ… visualizations saved to", VIZ_DIR)

9. å¾Œè™•ç† (Post-processing)
å¦‚æœä½ çš„è©•è«–æ˜¯å¤šåœ‹èªè¨€ï¼ˆZH / EN / ID / VNï¼‰ï¼Œå¯ä»¥åœ¨é€™æ­¥ï¼š
ä¸€ã€åŠ èªè¨€åµæ¸¬ â†’ éè‹±æ–‡å…ˆç¿»è­¯
äºŒã€æŠŠ neutral è‡ªå‹•åˆä½µåˆ° positive / negativeï¼Œæé«˜æ¨¡å‹ç©©å®šåº¦
ä¸‰ã€åšã€Œå“ç‰Œå±¤ç´šã€æˆ–ã€Œæ‰‹æ©Ÿå‹è™Ÿå±¤ç´šã€çš„å½™ç¸½
e.g. df.groupby('model_name')['sentiment'].mean()
å¯ä»¥ç”¢ä¸€å¼µ visualizations/sentiment_by_model.png
ä¹Ÿå¯ä»¥åœ¨é€™æ­¥ç”¢å‡ºä¸€å€‹ report.csvï¼š
review_id, raw_text, clean_text, predicted_sentiment, probability, model_version
...
é€™æ¨£ä½ åšå‰ç«¯é é¢å°±èƒ½ç›´æ¥åƒã€‚

10. Demo / Web åµŒå…¥å»ºè­°
ä½ èªªä½ è¦ã€ŒæŠŠä»–ç•¶ demo åšæˆç¶²é ã€ï¼Œå¯ä»¥ç…§é€™å€‹åšæ³•ï¼š
ä¸€ã€å¾Œç«¯å…ˆè·‘å¥½ä¸Šé¢çš„ pipelineï¼ŒæŠŠåœ–ç‰‡éƒ½è¼¸å‡ºåˆ° visualizations/
äºŒã€å‰ç«¯é é¢ç›´æ¥è®€å›ºå®šè·¯å¾‘çš„åœ–
/visualizations/sentiment_distribution.png
/visualizations/confusion_matrix.png
ä¸‰ã€å†åšä¸€å€‹ç°¡å–®çš„è¼¸å…¥æ¡†ï¼Œå‘¼å« 05_inference.py æ›¿ä½ åšå³æ™‚é æ¸¬

11. Inferenceï¼ˆå³æ™‚é æ¸¬ï¼‰
ğŸ“„ ç¯„ä¾‹ï¼ˆ05_inference.pyï¼‰ï¼š
import argparse
import joblib

bundle = joblib.load("models/sentiment_model.pkl")
model = bundle["model"]
vectorizer = bundle["vectorizer"]

def predict(text: str):
    x = vectorizer.transform([text])
    pred = model.predict(x)[0]
    proba = getattr(model, "predict_proba", None)
    if proba:
        p = model.predict_proba(x).max()
    else:
        p = None
    return pred, p

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--text", required=True, help="review text to classify")
    args = parser.parse_args()
    label, prob = predict(args.text)
    print(f"ğŸ”® Sentiment: {label} (prob={prob})")

12. git æŒ‡ä»¤ï¼ˆè‡ªå·± pushï¼‰
é€™æ¨£ä¸‹å°±å¥½ï¼š
git add README.md scripts/*.py visualizations/*.png
git commit -m "add detailed README and pipeline scripts"
git push origin main

Author: Rita Lin
Contact with me: msmile09@hotmail.com
Website: http://ritalinyutzu.vercel.app/
